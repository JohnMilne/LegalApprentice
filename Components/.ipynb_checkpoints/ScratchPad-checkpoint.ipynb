{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Keras model loading and saving examples\n",
    "# from keras.models import load_model\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# model = load_model('save_test.h5')\n",
    "# model.summary()\n",
    "\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "\n",
    "# model = Sequential();\n",
    "# model.add(Dense(32, input_dim=10))\n",
    "# model.add(Dense(10, activation='relu'))\n",
    "# model.add(Dense(5, activation='sigmoid'))\n",
    "\n",
    "# model.compile(optimizer='adam',loss='mse')\n",
    "# model.summary()\n",
    "\n",
    "# model.save('save_test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I still need to learn what kind of input to give the multilabel confusion matrix before I can uncomment below:\n",
    "    \n",
    "# # Now to produce a multilabel confusion matrix, which shows the binary confusion matrix for each <label> against all the\n",
    "# # other labels, which are binned together into one label <other> (in order to have a binary confusion matrix at all!).\n",
    "# cv_con_mat = multilabel_confusion_matrix(y_test_1_hot,\n",
    "#                                          cv_predictions_per_class)\n",
    "# # I am using Seaborn's heatmap function to plot the multilabel confusion matrices.\n",
    "# sns.heatmap(cv_con_mat,\n",
    "#             cmap   = 'coolwarm',\n",
    "#             annot  = True,\n",
    "#             square = True);\n",
    "# plt.title(f'CountVectorizer Confusion Matrices for Model #{new_model_number}')\n",
    "\n",
    "# # And now to do the same for the TF-IDF model\n",
    "# tf_con_mat = multilabel_confusion_matrix(y_test_1_hot,\n",
    "#                                          tf_predictions_per_class)\n",
    "# sns.heatmap(tf_con_mat,\n",
    "#             cmap   = 'coolwarm',\n",
    "#             annot  = True,\n",
    "#             square = True);\n",
    "# plt.title(f'TF-IDF Confusion Matrices for Model #{new_model_number}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# endpoints = defaultdict(list)\n",
    "# for i in range(len(cv_models['Dropout'])):\n",
    "#     if i == (len(cv_models['Dropout']) - 1):\n",
    "#         break\n",
    "#     if cv_models['Dropout'][i] != cv_models['Dropout'][i+1]:\n",
    "#         endpoints[cv_models['Dropout'][i]].append(i)\n",
    "# endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# endpoints2 = defaultdict(list)\n",
    "# for i in range(len(cv_models['Model #'])):\n",
    "#     if i == (len(cv_models['Model #']) - 1):\n",
    "#         break\n",
    "#     if cv_models['Model #'][i] != cv_models['Model #'][i+1]:\n",
    "#         endpoints2[cv_models['Model #'][i]].append(i)\n",
    "# endpoints2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(int(tf_models['Model #'].unique()[0]),\n",
    "#                int(tf_models['Model #'].unique()[-2])):\n",
    "#     label = f'TF-IDF{int(i)}'\n",
    "#     plt.plot(np.arange(1,(1 + len(tf_models['Model #'][tf_models['Model #'] == i]))),\n",
    "#              tf_models['Testing Accuracy'][tf_models['Model #'] == i],\n",
    "#              linewidth = 3,\n",
    "#              label     = label);\n",
    "#     plt.annotate(label,\n",
    "#                  ((((tf_models['Testing Accuracy'][tf_models['Model #'] == i]).idxmax()) - \n",
    "#                   ((tf_models['Testing Accuracy'][tf_models['Model #'] == i]).idxmin())),\n",
    "#                   np.amax(tf_models['Testing Accuracy'][tf_models['Model #'] == i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-3e944c54ec45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msysconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# print(tf.__version__)\n",
    "# print(tf.sysconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # This pickle creator function was developed for the Legal Apprentice workflow,\n",
    "# # written by John Milne, 10/17/2019\n",
    "\n",
    "# # This function takes the JSON-formatted data in /Data that the Legal\n",
    "# # Apprentice data starts as, throws it into a dataframe and then pickles that\n",
    "# # to the /Pickle directory.\n",
    "\n",
    "# # The assumption here is that the data stored at \"~/.Data/\" hasn't previously\n",
    "# # been pickled and the running of this function is to create the pickle file\n",
    "# # of the JSON-formatted NEW data, which is then moved to the /Pickle directory;\n",
    "# # thus, the \"./Data/\" directory only contains that data which needs to be\n",
    "# # pickled and nothing else and any data in the /Pickle directory has either\n",
    "# # been moved or renamed so as not to overwrite any previous work (that should\n",
    "# # not be overwritten).\n",
    "\n",
    "# def legal_apprentice_pickler():\n",
    "    \n",
    "#     # Imports of import.\n",
    "#     import json\n",
    "#     import os\n",
    "#     import pandas as pd  \n",
    "        \n",
    "#     # Creating the dataframe into which all of the files will be stored:\n",
    "#     df = pd.DataFrame()\n",
    "    \n",
    "#     # Getting the list of files in <data_path>:\n",
    "#     data_path = './Data/'\n",
    "#     list_of_files = os.listdir(data_path)\n",
    "    \n",
    "#     # Using a for-loop to iterate over the filenames...\n",
    "#     for filename in list_of_files:\n",
    "        \n",
    "#         # ... and opening the given filename...\n",
    "#         file = open(data_path + filename)\n",
    "        \n",
    "#         # ...using the json file loader to translate the json data...\n",
    "#         data = json.load(file)\n",
    "        \n",
    "#         # ...and creating new lists for the texts of the sentences...\n",
    "#         df_sents = []\n",
    "#         df_rhets = []\n",
    "        \n",
    "#         # ...and adding the sentences to those new lists...\n",
    "#         for sent in data['sentences']:\n",
    "            \n",
    "#             # ...creating the 'Sentences'...\n",
    "#             df_sents.append(sent['text'])\n",
    "            \n",
    "#             # ...and the 'RhetoricalRoles' columns...\n",
    "#             df_rhets.append(sent['rhetRole'][0])\n",
    "            \n",
    "#     # ...and adding those to the previously instantiated dataframe:\n",
    "#     df['Sentences']       = df_sents\n",
    "#     df['RhetoricalRoles'] = df_rhets\n",
    "                \n",
    "#     # Pickling the dataframe:\n",
    "#     df.to_pickle(\"./Pickles/50Cases.pkl\")\n",
    "    \n",
    "#     # Now to pass the fact that this has completed as the return statement:\n",
    "#     pickled = 'Done'\n",
    "    \n",
    "#     return pickled\n",
    "\n",
    "# legal_apprentice_pickler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.python'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-ac04540ef42d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msplit_and_pickled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m \u001b[0mnlp_transformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'counts'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-ac04540ef42d>\u001b[0m in \u001b[0;36mnlp_transformer\u001b[1;34m(max_words, mode, ngrams)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m    \u001b[1;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmulticlass\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m              \u001b[1;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlosses_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\utils\\conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mset_epsilon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfloatx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mset_floatx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcast_to_floatx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\load_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tensorflow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;31m# Try and load external backend.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtfdev\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.python'"
     ]
    }
   ],
   "source": [
    "# The nlp_transformer function was developed for the Legal Apprentice workflow,\n",
    "# written by John Milne, 10/15/2019\n",
    "\n",
    "# This function takes as an input the data from the Legal Apprentice workflow\n",
    "# that is assumed to have been saved to \"~/Pickle/\" as the currently known\n",
    "# pickle file name of 50Cases.pkl.\n",
    "\n",
    "# Another assumption is that the data will be a dataframe that has at least\n",
    "# two columns which are labeled as Sentences and RhetoricalRoles respectively.\n",
    "\n",
    "# This function will transform the data with the Keras' Tokenizer transformer\n",
    "# using one of the available <modes> (default is count) of transformation and\n",
    "# will pickle those transformed datasets (X_train, X_test, y_train, y_test)\n",
    "# into 4 pickle files holding all 4 transformed training and testing datasets.\n",
    "\n",
    "# This is an NLP transformer process.  The passed variables are the\n",
    "# hyperparameters of the NLP transformer.  The max_words variable determines\n",
    "# the maximum number of words to keep within the transformer, the ngrams tuple\n",
    "# gives the minimum and maximum (respectively) of the number of consecutive\n",
    "# words to pay attention to and the mode refers to the type of NLP transformer\n",
    "# being used.  The current set of modes available in Tokenizer are 'binary',\n",
    "# 'count', 'freq' and 'tf-idf'.\n",
    "\n",
    "def nlp_transformer(max_words = 5000,\n",
    "                    mode      = 'count',\n",
    "                    ngrams    = (1,3)):\n",
    "    \n",
    "    # Necessary imports:\n",
    "    from sklearn.model_selection  import train_test_split\n",
    "    from sklearn.preprocessing    import LabelEncoder\n",
    "    from sklearn.utils.multiclass import unique_labels\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    from python.keras.utilsimport to_categorical\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    # Ingesting the data from \"~/Pickle/50cases.pkl\"\n",
    "    df = pd.read_pickle(\"./Pickle/50Cases.pkl\")\n",
    "    \n",
    "    # Using train_test_split to do the sorting into training and testing\n",
    "    # datasets.  The random_state flag allows for reproducability across\n",
    "    # implementations, only using a 10% testing split due to a low amount of\n",
    "    # data currently and there is the need to set the shuffle flag to false to\n",
    "    # accomplish that reproducability.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.Sentences,\n",
    "                                                        df.RhetoricalRoles,\n",
    "                                                        random_state = 42,\n",
    "                                                        test_size    = 0.1,\n",
    "                                                        shuffle      = False)\n",
    "    \n",
    "    # Instantiating the Tokenizer object with the passed max_words variable.\n",
    "    tokens = Tokenizer(num_words = max_words)\n",
    "    \n",
    "    ### The actual fit/transform on the training data.\n",
    "    \n",
    "    # Step #1 is to use the fit_on_text to transform the tokenizer using the\n",
    "    # training data.\n",
    "    tokens.fit_on_text(X_train)\n",
    "    \n",
    "    # Step #2 is to use the text_to_matrix method on both the training and\n",
    "    # testing data, passing mode as the NLP transform type.\n",
    "    X_train_tokens = tokens.text_to_matrix(X_train,\n",
    "                                           mode = mode) \n",
    "    X_test_tokens  = tokens.text_to_matrix(X_test,\n",
    "                                           mode = mode)\n",
    "    \n",
    "    # Turning the labels on the training data into one-hot-encoded vectors that\n",
    "    # the neural network will understand.\n",
    "    \n",
    "    # First step is to use Sci-Kit Learn's labelEncoder to turn the text labels\n",
    "    # into integers:\n",
    "    \n",
    "    # Initializing the LabelEncoder:\n",
    "    encoder = LabelEncoder()\n",
    "    \n",
    "    # The LabelEncoder is fit to the y_train labels...\n",
    "    encoder.fit(y_train)\n",
    "    \n",
    "    # ...and then used to transform the y_* series of labels.\n",
    "    y_train_encode = encoder.transform(y_train)\n",
    "    y_test_encode  = encoder.transform(y_test)\n",
    "    \n",
    "    # The second step is to one-hot-encode those integer-based vectors using\n",
    "    # Sci-Kit Learn's to_categorical function.\n",
    "    y_train_1_hot = to_categorical(y_train_encode)\n",
    "    y_test_1_hot  = to_categorical(y_test_encode)\n",
    "    \n",
    "    # Now that the train/test/split is complete, pickling the transformed\n",
    "    # datasets into the respective /Training and /Testing directories:\n",
    "    X_train_tokens.to_pickle(\"./Pickles/Training/X_train.pkl\")\n",
    "    X_test_tokens.to_pickle(\"./Pickles/Testing/X_test.pkl\")\n",
    "    y_train_1_hot.to_pickle(\"./Pickles/Training/y_train.pkl\")\n",
    "    y_test_1_hot.to_pickle(\"./Pickles/Testing/y_test.pkl\")\n",
    "    \n",
    "    # Now to pass the fact that this has completed as the return statement:\n",
    "    split_and_pickled = 'Done'\n",
    "    \n",
    "    return split_and_pickled\n",
    "\n",
    "nlp_transformer(mode = 'counts')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Keras model loading and saving examples\n",
    "# from keras.models import load_model\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# model = load_model('save_test.h5')\n",
    "# model.summary()\n",
    "\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "\n",
    "# model = Sequential();\n",
    "# model.add(Dense(32, input_dim=10))\n",
    "# model.add(Dense(10, activation='relu'))\n",
    "# model.add(Dense(5, activation='sigmoid'))\n",
    "\n",
    "# model.compile(optimizer='adam',loss='mse')\n",
    "# model.summary()\n",
    "\n",
    "# model.save('save_test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I still need to learn what kind of input to give the multilabel confusion matrix before I can uncomment below:\n",
    "    \n",
    "# # Now to produce a multilabel confusion matrix, which shows the binary confusion matrix for each <label> against all the\n",
    "# # other labels, which are binned together into one label <other> (in order to have a binary confusion matrix at all!).\n",
    "# cv_con_mat = multilabel_confusion_matrix(y_test_1_hot,\n",
    "#                                          cv_predictions_per_class)\n",
    "# # I am using Seaborn's heatmap function to plot the multilabel confusion matrices.\n",
    "# sns.heatmap(cv_con_mat,\n",
    "#             cmap   = 'coolwarm',\n",
    "#             annot  = True,\n",
    "#             square = True);\n",
    "# plt.title(f'CountVectorizer Confusion Matrices for Model #{new_model_number}')\n",
    "\n",
    "# # And now to do the same for the TF-IDF model\n",
    "# tf_con_mat = multilabel_confusion_matrix(y_test_1_hot,\n",
    "#                                          tf_predictions_per_class)\n",
    "# sns.heatmap(tf_con_mat,\n",
    "#             cmap   = 'coolwarm',\n",
    "#             annot  = True,\n",
    "#             square = True);\n",
    "# plt.title(f'TF-IDF Confusion Matrices for Model #{new_model_number}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# endpoints = defaultdict(list)\n",
    "# for i in range(len(cv_models['Dropout'])):\n",
    "#     if i == (len(cv_models['Dropout']) - 1):\n",
    "#         break\n",
    "#     if cv_models['Dropout'][i] != cv_models['Dropout'][i+1]:\n",
    "#         endpoints[cv_models['Dropout'][i]].append(i)\n",
    "# endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# endpoints2 = defaultdict(list)\n",
    "# for i in range(len(cv_models['Model #'])):\n",
    "#     if i == (len(cv_models['Model #']) - 1):\n",
    "#         break\n",
    "#     if cv_models['Model #'][i] != cv_models['Model #'][i+1]:\n",
    "#         endpoints2[cv_models['Model #'][i]].append(i)\n",
    "# endpoints2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(int(tf_models['Model #'].unique()[0]),\n",
    "#                int(tf_models['Model #'].unique()[-2])):\n",
    "#     label = f'TF-IDF{int(i)}'\n",
    "#     plt.plot(np.arange(1,(1 + len(tf_models['Model #'][tf_models['Model #'] == i]))),\n",
    "#              tf_models['Testing Accuracy'][tf_models['Model #'] == i],\n",
    "#              linewidth = 3,\n",
    "#              label     = label);\n",
    "#     plt.annotate(label,\n",
    "#                  ((((tf_models['Testing Accuracy'][tf_models['Model #'] == i]).idxmax()) - \n",
    "#                   ((tf_models['Testing Accuracy'][tf_models['Model #'] == i]).idxmin())),\n",
    "#                   np.amax(tf_models['Testing Accuracy'][tf_models['Model #'] == i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n",
      "<module 'tensorflow._api.v1.sysconfig' from 'C:\\\\Users\\\\607593\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\lib\\\\site-packages\\\\tensorflow\\\\_api\\\\v1\\\\sysconfig\\\\__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.sysconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # This pickle creator function was developed for the Legal Apprentice workflow,\n",
    "# # written by John Milne, 10/17/2019\n",
    "\n",
    "# # This function takes the JSON-formatted data in /Data that the Legal\n",
    "# # Apprentice data starts as, throws it into a dataframe and then pickles that\n",
    "# # to the /Pickle directory.\n",
    "\n",
    "# # The assumption here is that the data stored at \"~/.Data/\" hasn't previously\n",
    "# # been pickled and the running of this function is to create the pickle file\n",
    "# # of the JSON-formatted NEW data, which is then moved to the /Pickle directory;\n",
    "# # thus, the \"./Data/\" directory only contains that data which needs to be\n",
    "# # pickled and nothing else and any data in the /Pickle directory has either\n",
    "# # been moved or renamed so as not to overwrite any previous work (that should\n",
    "# # not be overwritten).\n",
    "\n",
    "# def legal_apprentice_pickler():\n",
    "    \n",
    "#     # Imports of import.\n",
    "#     import json\n",
    "#     import os\n",
    "#     import pandas as pd  \n",
    "        \n",
    "#     # Creating the dataframe into which all of the files will be stored:\n",
    "#     df = pd.DataFrame()\n",
    "    \n",
    "#     # Getting the list of files in <data_path>:\n",
    "#     data_path = './Data/'\n",
    "#     list_of_files = os.listdir(data_path)\n",
    "    \n",
    "#     # Using a for-loop to iterate over the filenames...\n",
    "#     for filename in list_of_files:\n",
    "        \n",
    "#         # ... and opening the given filename...\n",
    "#         file = open(data_path + filename)\n",
    "        \n",
    "#         # ...using the json file loader to translate the json data...\n",
    "#         data = json.load(file)\n",
    "        \n",
    "#         # ...and creating new lists for the texts of the sentences...\n",
    "#         df_sents = []\n",
    "#         df_rhets = []\n",
    "        \n",
    "#         # ...and adding the sentences to those new lists...\n",
    "#         for sent in data['sentences']:\n",
    "            \n",
    "#             # ...creating the 'Sentences'...\n",
    "#             df_sents.append(sent['text'])\n",
    "            \n",
    "#             # ...and the 'RhetoricalRoles' columns...\n",
    "#             df_rhets.append(sent['rhetRole'][0])\n",
    "            \n",
    "#     # ...and adding those to the previously instantiated dataframe:\n",
    "#     df['Sentences']       = df_sents\n",
    "#     df['RhetoricalRoles'] = df_rhets\n",
    "                \n",
    "#     # Pickling the dataframe:\n",
    "#     df.to_pickle(\"./Pickles/50Cases.pkl\")\n",
    "    \n",
    "#     # Now to pass the fact that this has completed as the return statement:\n",
    "#     pickled = 'Done'\n",
    "    \n",
    "#     return pickled\n",
    "\n",
    "# legal_apprentice_pickler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CitationSentence', 'EvidenceSentence', 'FindingSentence',\n",
       "       'LegalRuleSentence', 'ReasoningSentence', 'Sentence'], dtype='<U17')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The nlp_transformer function was developed for the Legal Apprentice workflow,\n",
    "# written by John Milne, 10/15/2019\n",
    "\n",
    "# This function takes as an input the data from the Legal Apprentice workflow\n",
    "# that is assumed to have been saved to \"~/Pickles/\" as the currently known\n",
    "# pickle file name of 50Cases.pkl.\n",
    "\n",
    "# Another assumption is that the data will be a dataframe that has at least\n",
    "# two columns which are labeled as Sentences and RhetoricalRoles respectively.\n",
    "\n",
    "# This function will do a train/test/split on the data, then transform the\n",
    "# data with the Keras' Tokenizer transformer using one of the available\n",
    "# <modes> (default is count) of transformation and pickle those\n",
    "# split-transformed datasets (X_train, X_test, y_train, y_test) into 4 pickle\n",
    "# files holding all 4 transformed training and testing datasets.\n",
    "\n",
    "# This is an NLP transformer process.  The passed variables are the\n",
    "# hyperparameters of the NLP transformer.  The max_words variable determines\n",
    "# the maximum number of words to keep within the transformer, the ngrams tuple\n",
    "# gives the minimum and maximum (respectively) of the number of consecutive\n",
    "# words to pay attention to and the mode refers to the type of NLP transformer\n",
    "# being used.  The current set of modes available in Tokenizer are 'binary',\n",
    "# 'count', 'freq' and 'tf-idf'.\n",
    "\n",
    "# The function will return the list of unique labels if such are needed\n",
    "# further along in the workflow.\n",
    "\n",
    "def nlp_transformer(max_words = 5000,\n",
    "                    mode      = 'count',\n",
    "                    ngrams    = (1,3)):\n",
    "    \n",
    "    # Necessary imports:\n",
    "    from sklearn.model_selection  import train_test_split\n",
    "    from sklearn.preprocessing    import LabelEncoder\n",
    "    from sklearn.utils.multiclass import unique_labels\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    from keras.utils              import to_categorical\n",
    "    \n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    \n",
    "    # Ingesting the data from \"~/Pickles/50cases.pkl\"\n",
    "    df = pd.read_pickle(\"./Pickles/50Cases.pkl\")\n",
    "    \n",
    "    # Using train_test_split to do the sorting into training and testing\n",
    "    # datasets.  The random_state flag allows for reproducability across\n",
    "    # implementations, only using a 15% testing split due to a low amount of\n",
    "    # data currently and there is the need to set the shuffle flag to false to\n",
    "    # accomplish that reproducability.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.Sentences,\n",
    "                                                        df.RhetoricalRoles,\n",
    "                                                        random_state = 42,\n",
    "                                                        test_size    = 0.15,\n",
    "                                                        shuffle      = False)\n",
    "    \n",
    "    # Instantiating the Tokenizer object with the passed max_words variable.\n",
    "    tokens = Tokenizer(num_words = max_words)\n",
    "    \n",
    "    ### The actual fit/transform on the training data.\n",
    "    \n",
    "    # Step #1 is to use the fit_on_text to transform the tokenizer using the\n",
    "    # training data.\n",
    "    tokens.fit_on_texts(X_train)\n",
    "    \n",
    "    # Step #2 is to use the text_to_matrix method on both the training and\n",
    "    # testing data, passing mode as the NLP transform type.\n",
    "    X_train_tokens = tokens.texts_to_matrix(X_train,\n",
    "                                            mode = mode) \n",
    "    X_test_tokens  = tokens.texts_to_matrix(X_test,\n",
    "                                            mode = mode)\n",
    "    \n",
    "    # Turning the labels on the training data into one-hot-encoded vectors that\n",
    "    # the neural network will understand.\n",
    "    \n",
    "    # First step is to use Sci-Kit Learn's labelEncoder to turn the text labels\n",
    "    # into integers:\n",
    "    \n",
    "    # Initializing the LabelEncoder:\n",
    "    encoder = LabelEncoder()\n",
    "    \n",
    "    # The LabelEncoder is fit to the y_train labels...\n",
    "    encoder.fit(y_train)\n",
    "    \n",
    "    # ...and then used to transform the y_* series of labels.\n",
    "    y_train_encode = encoder.transform(y_train)\n",
    "    y_test_encode  = encoder.transform(y_test)\n",
    "    \n",
    "    # The second step is to one-hot-encode those integer-based vectors using\n",
    "    # Sci-Kit Learn's to_categorical function.\n",
    "    y_train_1_hot = to_categorical(y_train_encode)\n",
    "    y_test_1_hot  = to_categorical(y_test_encode)\n",
    "    \n",
    "    # Now that the train/test/split is complete, pickling the transformed\n",
    "    # datasets into the respective /Training and /Testing directories:\n",
    "    \n",
    "    # Using try/except in order to not error out when the file already exists\n",
    "    try:\n",
    "        with open('./Pickles/Training/X_train.pkl','xb') as f:\n",
    "            pickle.dump(X_train_tokens, f)\n",
    "    except FileExistsError:\n",
    "        print(\"the file X_train already exists - rename or move and retry.\")\n",
    "\n",
    "    try:\n",
    "        with open('./Pickles/Testing/X_test.pkl','xb') as f:\n",
    "            pickle.dump(X_test_tokens, f)\n",
    "    except FileExistsError:\n",
    "        print(\"The file X_test already exists - rename or move and retry.\")\n",
    "\n",
    "    try:\n",
    "        with open('./Pickles/Training/y_train.pkl','xb') as f:\n",
    "            pickle.dump(y_train_1_hot, f)\n",
    "    except FileExistsError:\n",
    "        print(\"The file y_train already exists - rename or move and retry.\")\n",
    "\n",
    "    try:\n",
    "        with open('./Pickles/Testing/y_test.pkl','xb') as f:\n",
    "            pickle.dump(y_test_1_hot, f)\n",
    "    except FileExistsError:\n",
    "        print(\"The file y_test already exists - rename or move and retry.\")\n",
    "        \n",
    "    # Creating the unique labels list as the return item:\n",
    "    labels = unique_labels(y_test)\n",
    "    \n",
    "    return labels\n",
    "\n",
    "nlp_transformer(mode = 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to create file (unable to open file: name = './model_saves/model.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 302)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-9a989eca160f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmax_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m \u001b[0mmodel_compiler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-9a989eca160f>\u001b[0m in \u001b[0;36mmodel_compiler\u001b[1;34m(max_words, dropout, labels, reduce, scale)\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[1;31m# Saving the model.  This saves the weights and biases of each node as well\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[1;31m# as the optimizer used when compiling the model:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./model_saves/model.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;31m# This gargantuan model will produce a very interesting summary; thus, the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[0;32m   1088\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1089\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1090\u001b[1;33m         \u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1091\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1092\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36msave_model\u001b[1;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[0mopened_new_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 379\u001b[1;33m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\utils\\io_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, mode)\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[0;32m    392\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[0;32m    393\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m                                swmr=swmr)\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[1;31m# Open in append mode (read/write).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to create file (unable to open file: name = './model_saves/model.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 302)"
     ]
    }
   ],
   "source": [
    "# The model_compiler function was developed for the Legal Apprentice workflow,\n",
    "# written by John Milne, 10/15/2019\n",
    "\n",
    "# This compiles the neural network model layers.  This compiler does not do any\n",
    "# training on training data or any predicting on test data.  It just creates\n",
    "# the model using the following passed parameters:\n",
    "#   max_words - default of 5000\n",
    "#   dropout   - default of 0.50\n",
    "#   labels    - default of ['CitationSentence','EvidenceSentence',\n",
    "#                           'FindingSentence','LegalRuleSentence',\n",
    "#                           'ReasoningSentence','Sentence'],\n",
    "#   reduction - default of 1\n",
    "#   scale     - default of 1\n",
    "\n",
    "# The meaning is those constants is as follows:\n",
    "#   max_words is the maximum number of stored features used by the model.  This\n",
    "#       is also the number passed by the nlp_transformer function that should\n",
    "#       have been used to create the data; so, assuming this is correct, then\n",
    "#       this can be passed as max_words = <nlp_transformer>[2]\n",
    "#   dropout is the regularization rate of the model - use the default unless\n",
    "#       specifically testing new models.\n",
    "#   labels is the list of labels that are the answers for which type of\n",
    "#       sentence the model is going to label each sentence.  Labels is also\n",
    "#       the output of the nlp_transformer function, which if called previously\n",
    "#       can be passed as labels = <nlp_transformer>[1]\n",
    "#   reduce is a scaler used within the model to scale the layers' nodes\n",
    "#       compared to the input layer - use default unless testing again.\n",
    "#   scale is another scaler for the model - again use default unless testing.\n",
    "\n",
    "### Note: When the production version of this is created, these should become\n",
    "#   hard-coded constants associated with the best performing model within the\n",
    "#   framework of the function rather than as passed variables as this function\n",
    "#   is currently built.  They are currently only set up as passed variables for\n",
    "#   ease of use when testing out different models.\n",
    "\n",
    "def model_compiler(max_words = 5000,\n",
    "                   dropout   = 0.50,\n",
    "                   labels    = ['CitationSentence','EvidenceSentence',\n",
    "                                'FindingSentence','LegalRuleSentence',\n",
    "                                'ReasoningSentence','Sentence'],\n",
    "                   reduce    = 2,\n",
    "                   scale     = 2):\n",
    "    \n",
    "    # Imports of import:\n",
    "    from keras.layers import Dense, Dropout\n",
    "    from keras.models import Sequential, save_model\n",
    "    \n",
    "    # The first step of the model creation is to instantiate the model:\n",
    "    model = Sequential();\n",
    "\n",
    "    # Adding the layers:\n",
    "\n",
    "    # The first layer is the input layer.  It seems desirable to have the\n",
    "    # number of input nodes equal to the size of the input data.  The input\n",
    "    # data is sized by the max_words constant initialized at the beginning of\n",
    "    # the nlp_transformer function prior to this function's use.  The default\n",
    "    # for max_words is 5000, which is an extremely large input layer when going\n",
    "    # with this method for creating said layer.  The activation type of 'relu'\n",
    "    # is the current data science best practice for activating nodes in a\n",
    "    # standard dense neural network.\n",
    "    model.add(Dense(max_words,\n",
    "                    input_shape = (max_words,),\n",
    "                    activation  = 'relu'))\n",
    "\n",
    "    # All subsequent layers between the input layer and the final output layer\n",
    "    # are the hidden layers.  Deep Learning's pedantic meaning is that for deep\n",
    "    # learning to be happening, at least one hidden layer must exist.\n",
    "    # Experience has shown that multiple hidden layers add to the magic - and\n",
    "    # the word magic means it is much more art than science on how the shape\n",
    "    # of the input layer versus the deep layers and the output layer affect\n",
    "    # the actual training of the model.  But, more deep layers typically brings\n",
    "    # about better performance against the accuracy metric commonly used with\n",
    "    # these standard dense neural networks.  Thus, this model will use multiple\n",
    "    # deep layers and part of the tweaking of the model during testing will be\n",
    "    # to change the size of the deep layers, so constants will be used\n",
    "    # associated with calculatable attributes from the data rather than\n",
    "    # hard-coding the number of nodes per layer.  The reduction constant allows\n",
    "    # for scaling of the first and fourth hidden layers by <reduction>.  The\n",
    "    # scale constant allows for further scaling of the second and third hidden\n",
    "    # layers.  Thus, if reduction and scale are both unity, then the hidden\n",
    "    # layers are the same size as the input layer and the shape is square.  If\n",
    "    # reduction is less than unity, then the hidden layers grow in size and if\n",
    "    # scale is greater than unity, those secondary hidden layers also grow in\n",
    "    # size.  This allows for the aforementioned square shape, a pear shape or\n",
    "    # a saddle shape in terms of the shape of the whole set of hidden layers\n",
    "    # when using the above constants.\n",
    "    model.add(Dense(int(max_words/reduce),\n",
    "                    activation = 'relu'))\n",
    "\n",
    "    # Overfitting refers to the condition where a model trains well against the\n",
    "    # training data, but doesn't actually understand the data well enough to\n",
    "    # perform well on new (testing) data.  This is referred to as variance,\n",
    "    # while the actual errors involved are referred to as bias. There is always\n",
    "    # a bias/variance trade-off which is exactly analogous to the Heisenberg\n",
    "    # Uncertainty Principle in physics.  Because of this, using dropout right\n",
    "    # from the start when building a neural network is typical because neural\n",
    "    # networks tend towards overfitting.  Dropout refers to the decimal\n",
    "    # percentage of the nodes in the previous layer that are randomly turned\n",
    "    # off during the current epoch of training while the neural network\n",
    "    # advances through its training epochs.  This necessarily introduces bias\n",
    "    # into the neural network, which trades off of the extra variance and\n",
    "    # thereby increases the performance of the neural network on testing data\n",
    "    # through the bias/variance trade-off (magic!).  This is referred to as\n",
    "    # regularization and is another parameter that can be tweaked to improve\n",
    "    # the neural network's overall performance.\n",
    "\n",
    "    # TL;DR: dropout is a decimal percentage and increases performance on\n",
    "    # testing data, which is the performance metric in use with this model.\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    # This is the second real hidden layer because the dropout layer is really\n",
    "    # a process vice a layer. The number of nodes are scaled by both the\n",
    "    # <scale> constant and the <reduction> constant, whereas, the previous\n",
    "    # hidden layer had only been scaled by the <reduction> constant.  If the\n",
    "    # <scale> constant equals 1, then the shape is square.  If the <scale>\n",
    "    # constant is positive, then the shape is pyramidal.  Lastly, if the\n",
    "    # <scale> constant is negative then we have a saddle shape.\n",
    "    model.add(Dense(int(max_words*scale/reduce),\n",
    "                    activation = 'relu'))\n",
    "\n",
    "    # Every hidden layer will have dropout applied to it.  The normal use of\n",
    "    # this is to preclude a particular node from becoming overactivated by an\n",
    "    # outlier in the data and becoming the driver of the output concerning that\n",
    "    # outlier.\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    # This is layer number 4 or hidden layer number 3.  Same as the previous\n",
    "    # layer.\n",
    "    model.add(Dense(int(max_words*scale/reduce),\n",
    "                    activation = 'relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    # The last hidden layer is the same as the first hidden layer, which allows\n",
    "    # for square, saddle and pear shapes as structures for the hidden layers.\n",
    "    model.add(Dense(int(max_words/reduce),\n",
    "                    activation = 'relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    # The Output Layer.  The output layer has two properties that distinguish\n",
    "    # it from the other layers.  The first distinction is that the number of\n",
    "    # nodes is dictated by the number of classes the neural network is trying\n",
    "    # to classify.  The second distinction is that the best practice in data\n",
    "    # science is to use softmax for the activation of the output layer vice\n",
    "    # the relu activation of the previous layers.\n",
    "    model.add(Dense(len(labels),\n",
    "                    activation = 'softmax'))\n",
    "\n",
    "    # That's it for the building of the neural network's layers.  The next step\n",
    "    # compiles them into the model object using the parameters given:\n",
    "    # categorical_crossentropy is the loss type for any neural network\n",
    "    # classification that is not a simple binary classification; 'adam' is the\n",
    "    # current best practice for the optimizers in neural networks - other\n",
    "    # options are SGD, RMSprop, Adagrad, Adadelta...  We are also most\n",
    "    # concerned about the accuracy (vice the recall or the specificity) of our\n",
    "    # model; thus, the metric being trained against is accuracy.\n",
    "    model.compile(loss      = 'categorical_crossentropy',\n",
    "                  optimizer = 'adam',\n",
    "                  metrics   = ['accuracy'])\n",
    "    \n",
    "    # Saving the model.  This saves the weights and biases of each node as well\n",
    "    # as the optimizer used when compiling the model:\n",
    "    model.save(\"./model_saves/model.h5\")\n",
    "    \n",
    "    # This gargantuan model will produce a very interesting summary; thus, the\n",
    "    # need to print out its summary as part of the output of the function:\n",
    "    model.summary() \n",
    "    \n",
    "    # And now to return the hyperparameters of the model:\n",
    "    return max_words, dropout, reduction, scale, labels\n",
    "\n",
    "model_compiler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
